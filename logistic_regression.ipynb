{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "\n",
    "xlsx_file = Path('train_2.xlsx')\n",
    "wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "sheet = wb_obj.active\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= np.zeros((sheet.max_row-1,sheet.max_column-1),float)\n",
    "Y_train=np.zeros((1,sheet.max_row-1),float)\n",
    "\n",
    "\n",
    "col_names = []\n",
    "\n",
    "c=0\n",
    "d=0\n",
    "\n",
    "\n",
    "for i in range(2,sheet.max_row+1):\n",
    "    \n",
    "    d=0\n",
    "    for j in range(1,sheet.max_column):\n",
    "        X_train[c,d]=sheet.cell(row=i,column=j).value\n",
    "        d=d+1\n",
    "      \n",
    "    if(sheet.cell(row=i,column=j+1).value=='Iris-virginica'):\n",
    "        Y_train[0,c]=0\n",
    "    else:\n",
    "        Y_train[0,c]=1\n",
    "        \n",
    "        \n",
    "    c=c+1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"test_2.csv\"\n",
    "fields = [] \n",
    "rows = [] \n",
    "c=0\n",
    "d=0  \n",
    "with open(filename, 'r') as csvfile: \n",
    "    csvreader = csv.reader(csvfile) \n",
    "      \n",
    "\n",
    "    fields = next(csvreader) \n",
    "  \n",
    "    for row in csvreader: \n",
    "        rows.append(row) \n",
    "  \n",
    "    # get total number of rows \n",
    "    #print(\"Total no. of rows: %d\"%(csvreader.line_num)) \n",
    "    \n",
    " \n",
    "#print('Field names are:' + ', '.join(field for field in fields)) \n",
    "X_test= np.zeros((csvreader.line_num-1,4),float)  \n",
    "Y_test=np.zeros((1,csvreader.line_num-1),float)\n",
    "\n",
    "for row in rows[:csvreader.line_num]: \n",
    "    d=0\n",
    "    # parsing each column of a row \n",
    "    for col in row:\n",
    "        if(d<=3):\n",
    "            X_test[c,d]=col\n",
    "        d=d+1\n",
    "        if(col=='Iris-virginica'):\n",
    "            Y_test[0,c]=0\n",
    "        else:\n",
    "            Y_test[0,c]=1    \n",
    "    c=c+1    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "    ### END CODE HERE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros(shape=(dim, 1))\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T,X.T) + b)  # compute activation\n",
    " \n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1 /m) * np.dot(X.T, (A - Y).T)\n",
    "    db = (1 /m) * np.sum(A - Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    #assert(dw.shape == w.shape)\n",
    "    #assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    #assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "  \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "    \n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw  # need to broadcast\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "       \n",
    "        if (i % 1000 == 0):\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m=X.shape[0]\n",
    "    Y_prediction = np.zeros((m,1))\n",
    "    w = w.reshape(X.shape[1], 1)\n",
    "\n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T,X.T) + b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        Y_prediction[i,0] = 1 if A[0,i] > 0.5 else 0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (m,1))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    dim=X_train.shape[0]\n",
    "    b=0\n",
    "    w=np.zeros([4,1])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    #print(Y_prediction_test)\n",
    "    \n",
    "  \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train.T - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test.T - Y_test)) * 100)) \n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 1000: 0.356650\n",
      "Cost after iteration 2000: 0.269098\n",
      "Cost after iteration 3000: 0.229757\n",
      "Cost after iteration 4000: 0.207224\n",
      "Cost after iteration 5000: 0.192475\n",
      "Cost after iteration 6000: 0.181978\n",
      "Cost after iteration 7000: 0.174066\n",
      "Cost after iteration 8000: 0.167851\n",
      "Cost after iteration 9000: 0.162812\n",
      "Cost after iteration 10000: 0.158625\n",
      "Cost after iteration 11000: 0.155077\n",
      "Cost after iteration 12000: 0.152019\n",
      "Cost after iteration 13000: 0.149349\n",
      "Cost after iteration 14000: 0.146989\n",
      "Cost after iteration 15000: 0.144884\n",
      "Cost after iteration 16000: 0.142989\n",
      "Cost after iteration 17000: 0.141271\n",
      "Cost after iteration 18000: 0.139702\n",
      "Cost after iteration 19000: 0.138261\n",
      "Cost after iteration 20000: 0.136931\n",
      "Cost after iteration 21000: 0.135698\n",
      "Cost after iteration 22000: 0.134549\n",
      "Cost after iteration 23000: 0.133474\n",
      "Cost after iteration 24000: 0.132466\n",
      "Cost after iteration 25000: 0.131517\n",
      "Cost after iteration 26000: 0.130620\n",
      "Cost after iteration 27000: 0.129772\n",
      "Cost after iteration 28000: 0.128967\n",
      "Cost after iteration 29000: 0.128200\n",
      "Cost after iteration 30000: 0.127470\n",
      "Cost after iteration 31000: 0.126772\n",
      "Cost after iteration 32000: 0.126105\n",
      "Cost after iteration 33000: 0.125465\n",
      "Cost after iteration 34000: 0.124851\n",
      "Cost after iteration 35000: 0.124260\n",
      "Cost after iteration 36000: 0.123691\n",
      "Cost after iteration 37000: 0.123143\n",
      "Cost after iteration 38000: 0.122614\n",
      "Cost after iteration 39000: 0.122102\n",
      "Cost after iteration 40000: 0.121607\n",
      "Cost after iteration 41000: 0.121128\n",
      "Cost after iteration 42000: 0.120664\n",
      "Cost after iteration 43000: 0.120213\n",
      "Cost after iteration 44000: 0.119776\n",
      "train accuracy: 94.36619718309859 %\n",
      "test accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(X_train,Y_train,X_test,Y_test, num_iterations =45000, learning_rate = 0.02, print_cost = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per thousands)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
